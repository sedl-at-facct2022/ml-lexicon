# Communication with Policy Makers and Other Communities as AI Researchers

This Lexicon Topic is inspired by the concepts discussed in the talk by [Andi Peng](https://andipeng.com/).

What members of other communities need to know to do *their job* can focus how researchers *translate* details and *distill* them into high level points.

Please add to these defintions with how your community may have differing defitions of these concepts.

### What is Artificial Intellgence?

The technical literacy gap is a meaningful challenge between communities.

- To AI researchers, AI provides researchers with frameworks for thinking about problems

- To policy makers, AI can appear to be magic. 

```{note}
How do members of your community or communities you work with understand AI?
```

### How do terms relate to each other?

- To AI researchers, concepts are clearly defined and organized.

- To policy makers, separation between concepts can be unclear.

```{note}
How do members of your community or communities define and deliniate between concepts?
```

### Specialization/Generalization

- Specialists care about the details of a specific problem.  AI researchers are specialists.

- Generalists care about big-picture ideas and impact. Policy makers are genearlists.

```{note}
How do members of your community or communities emphasize specialization and genearlization?
```

## Normative/Positive Use of AI

- Normative questions in AI Include:

    - How do we use good AI?
    - How shouldn't we use bad AI?
    - How do we do more good AI?

- Positive questions in AI include:
    
    - What is AI?
    - What does AI do well?
    - What does AI do poorly?

Policiy makers tend to focus on normative questions, while AI researchers tend to focus on positive questions.

```{note}
What positive and nomative questions are important to your community and the communties you work with?
```

### Differences in Question Formulation

Questions can differ between communities.  

A researcher may ask:

> Are individual and group level metrics creating disparate impacts?

A policy maker may ask:

> Is this model better than a human?

Similarly:

> Does this model exhibit the structure and capabilities of human intelligence?

May translate to:

> Can we treat this model like a human?

```{note}
How do technical questions in AI translate to your community or communities you collaborate with?
```